X <- red_dim$x[,c(1,2)]
O <- O %*% rot_var
}
dimnames(O)[[1]] = dimnames(Y)[[1]] <- dimnames(X)[[1]]  # Keeping same sample names
dimnames(O)[[2]] <- dimnames(X)[[2]] # Keeping same feature names
D <- matrix((X-O)^2 %*% matrix(rep(1, p), ncol = 1))  # Euclidean distance calculation
D <- order(D,decreasing = F)[1:k]  # Filtering k-nearest neighbours
if(is.numeric(df[,response])){  # regression mode
obs[,response] <- mean(Y[D,])
return(obs)
} else{  # classification mode
votes <- table(Y[D])
# picking randomly a choice if equal number of choices
obs[,response] <- names(votes[votes==max(votes)])
prop_votes <- votes[obs[,response]]/sum(votes)
return(list(class=obs, confidence=prop_votes))
}
}
pred_class <- list(class=rep(NA,nrow(iris)), conf=rep(NA,nrow(iris)))
for(i in 1:nrow(iris)){
pred_class$class[i] <- knn(df = iris[-i,], k = 4, obs = iris[i,], response = 'Species')$class[,'Species']
pred_class$conf[i] <- knn(df = iris[-i,], k = 10, obs = iris[i,], response = 'Species')$confidence
}
ped_cl
ped_class
pred_class
pred_class <- list(class=rep(NA,nrow(iris)), conf=rep(NA,nrow(iris)))
for(i in 1:nrow(iris)){
pred_class$class[i] <- knn(df = iris[-i,], k = 4, obs = iris[i,], response = 'Species')$class[,'Species']
print(knn(df = iris[-i,], k = 10, obs = iris[i,], response = 'Species')$confidence)
}
1:nrow(iris)
df=iris[-84,]
obs=iris[84,]
response='Species'
k=5
obs <- select(obs,-matches(response))
obs
isNum
isNum <- sapply(obs,is.numeric)
isNum
obs <- select(obs, which(isNum))
obs
num_features
num_features <- colnames(df)[unname(sapply(df,is.numeric))]  # Numeric variables only
num_features
num_features <- num_features[num_features != response]
num_features
N <- nrow(df)  # Number of observations
p <- length(num_features)  # Number of input variables
X <- as.matrix(df[,num_features])  # Transforming into matrix
O <- matrix(unlist(rep(unname(obs),N)),byrow=T,nrow=N)  # obs values as matrix
Y <- as.matrix(df[,response]) # Response variable as matrix
Y
dimnames(Y)[[2]] <- list(response)
dimnames(Y)
red_dim <- prcomp(X); p <- 2
red_dim
red_dim$rotation[,c(1,2)]
rot_var <- red_dim$rotation[,c(1,2)]
red_dim$x[,c(1,2)]
X <- red_dim$x[,c(1,2)]
O
obs
rot_vat
rot_var
O <- O %*% rot_var
O
dimnames(O)[[1]] = dimnames(Y)[[1]] <- dimnames(X)[[1]]  # Keeping same sample names
dimnames(O)[[2]] <- dimnames(X)[[2]] # Keeping same feature names
D <- matrix((X-O)^2 %*% matrix(rep(1, p), ncol = 1))  # Euclidean distance calculation
D <- matrix((X-O)^2 %*% matrix(rep(1, p), ncol = 1))  # Euclidean distance calculation
D <- order(D,decreasing = F)[1:k]  # Filtering k-nearest neighbours
D <- matrix((X-O)^2 %*% matrix(rep(1, p), ncol = 1))  # Euclidean distance calculation
D
max(D)
min(D)
df
prcomp(X)
red_dim$x[,c(1,2)]
red_dim$x[,c(1,2)]
red_dim$x[,c(1,2)]
O
O
source('~/Documents/git_projects/useless_things/algorithms_from_scratch/k-nn.R', echo=TRUE)
rot_var
O
rot_var
obs <- select(obs,-matches(response))
isNum <- sapply(obs,is.numeric)
obs <- select(obs, which(isNum))
num_features <- colnames(df)[unname(sapply(df,is.numeric))]  # Numeric variables only
num_features <- num_features[num_features != response]
N <- nrow(df)  # Number of observations
p <- length(num_features)  # Number of input variables
X <- as.matrix(df[,num_features])  # Transforming into matrix
O <- matrix(unlist(rep(unname(obs),N)),byrow=T,nrow=N)  # obs values as matrix
Y <- as.matrix(df[,response]) # Response variable as matrix
dimnames(Y)[[2]] <- list(response)
O
O
rot_var
plot(prcomp(iris[,-5])$x[,1],prcomp(iris[,-5])$x[,2])
plot(prcomp(iris[,-5])$x[,1],prcomp(iris[,-5],scale. = T)$x[,2])
plot(prcomp(iris[,-5], scale. = T)$x[,1],prcomp(iris[,-5],scale. = T)$x[,2])
plot(prcomp(iris[-84,-5], scale. = T)$x[,1],prcomp(iris[-84,-5],scale. = T)$x[,2])
PC12 <- prcomp(iris[-84,-5], scale. = T)$rotation
iris[84,-5] %*% PC12[,c(1,2)]
PC12[,c(1,2)]
matrix(iris[84,-5]) %*% PC12[,c(1,2)]
matrix(iris[84,-5])
t(matrix(iris[84,-5])) %*% PC12[,c(1,2)]
t(matrix(iris[84,-5]))
PC12[,c(1,2)]
test_val <- t(matrix(iris[84,-5]))
dimnames(test_val)[[1]] <- c("Sepal.Length", "Sepal.Width","Petal.Length","Petal.Width")
dimnames(test_val)[[2]] <- c("Sepal.Length", "Sepal.Width","Petal.Length","Petal.Width")
test_val %*% PC12[,c(1,2)]
test_val %*% matrix(PC12[,c(1,2)])
str(test_val)
test_val <- as.matrix(test_val)
str(test_val)
unlist(test_val)
unlist(test_val) %*% matrix(PC12[,c(1,2)])
matrix(rep(unlist(test_val),2),byrow=T,nrow=2) %*% matrix(PC12[,c(1,2)])
matrix(rep(unlist(test_val),2),byrow=T,nrow=2)
matrix(PC12[,c(1,2)])
matrix(rep(unlist(test_val),2),byrow=T,nrow=2) %*% PC12[,c(1,2)]
prcomp(iris[-84,-5],scale. = T)$x
plot(prcomp(iris[-84,-5], scale. = T, rank=2)$x[,1],prcomp(iris[-84,-5],scale. = T, rank. = 2)$x[,2])
prcomp(iris[-84,-5],scale. = T,rank=2)$x
prcomp(iris[84,-5],scale. = T,rank=2)$x
PC12 <- prcomp(iris[-84,-5], scale. = T, rank=2)$rotation
matrix(rep(unlist(test_val),2),byrow=T,nrow=2) %*% PC12[,c(1,2)]
prcomp(iris[,-5],scale. = T,rank=2)$x
matrix(rep(unlist(test_val),2),byrow=T,nrow=2) %*% PC12[,c(1,2)]
sample_test <-matrix(rep(unlist(test_val),2),byrow=T,nrow=2)
sample_test-colMeans(iris)
sample_test-colMeans(iris[,-5])
apply(iris,sd,MARGIN=2)
apply(iris[,-5],sd,MARGIN=2)
(sample_test-colMeans(iris[,-5])/apply(iris[,-5],sd,MARGIN=2)
)
(sample_test-colMeans(iris[,-5]))/apply(iris[,-5],sd,MARGIN=2)
sample_norm <-(sample_test-colMeans(iris[,-5]))/apply(iris[,-5],sd,MARGIN=2)
sample_norm %*% PC12[,c(1,2)]
sample_test-colMeans(iris[,-5])
sample_test-colMeans(iris[,-5])
colMeans(iris[,-5])
apply(sample_test, MARGIN = 1,function(x) x-colMeans(iris[,-5]))
apply(sample_test, MARGIN = 2,function(x) x-colMeans(iris[,-5]))
t(apply(sample_test, MARGIN = 1,function(x) x-colMeans(iris[,-5])))
sample_norm <- apply(sample_test, MARGIN = 2,function(x) x-colMeans(iris[,-5]))
sample_norm <- t(apply(sample_test, MARGIN = 2,function(x) x-colMeans(iris[,-5])))
sample_frac()
sample_norm
t(apply(sample_test, MARGIN = 2,function(x) x-colMeans(iris[,-5])))
t(apply(sample_test, MARGIN = 1,function(x) x-colMeans(iris[,-5])))
sample_norm <- t(apply(sample_test, MARGIN = 1,function(x) x-colMeans(iris[,-5])))
sd(iris)
apply(iris, , MARGIN=2, sd)
apply(iris, , MARGIN=2, FUN=sd)
apply(iris[,-5], , MARGIN=2, sd)
apply(iris[,-5], , MARGIN=2, FUN=sd)
apply(iris[,-5] , MARGIN=2, FUN=sd)
sample_norm/apply(iris[,-5] , MARGIN=2, FUN=sd)
apply(sample_norm, MARGIN=1, function(x) x/apply(iris[,-5] , MARGIN=2, FUN=sd))
t(apply(sample_norm, MARGIN=1, function(x) x/apply(iris[,-5] , MARGIN=2, FUN=sd)))
sample_norm <- t(apply(sample_norm, MARGIN=1, function(x) x/apply(iris[,-5] , MARGIN=2, FUN=sd)))
sapmle_norm %*% PC12
sample_norm %*% PC12
prcomp(iris[,-5], scale=T, rank=2)$x
ob
X
prcomp(X, scale = T)
colMeans(X)
t(apply(obs, MARGIN = 1,function(x) x-colMeans(X)))
obs <- t(apply(obs, MARGIN=1, function(x) x/apply(X , MARGIN=2, FUN=sd)))
obs
obs <- iris[84,]
obs <- select(obs,-matches(response))
obs <- select(obs, which(isNum))
obs <- t(apply(obs, MARGIN=1, function(x) x-colMeans(X)))
obs <- t(apply(obs, MARGIN=1, function(x) x/apply(X , MARGIN=2, FUN=sd)))
obs
O <- matrix(unlist(rep(unname(obs),N)),byrow=T,nrow=N)  # obs values as matrix
O %*% rot_var
library(tidyverse)
knn <- function(df, k, PCA=T, obs, response){
# implementing k-nearest neighbours. This function can work with
# categorical or numeric response. It can only predict one observation
# at a time.
obs <- select(obs,-matches(response))
isNum <- sapply(obs,is.numeric)
obs <- select(obs, which(isNum))
num_features <- colnames(df)[unname(sapply(df,is.numeric))]  # Numeric variables only
num_features <- num_features[num_features != response]
N <- nrow(df)  # Number of observations
p <- length(num_features)  # Number of input variables
X <- as.matrix(df[,num_features])  # Transforming into matrix
obs <- t(apply(obs, MARGIN=1, function(x) x-colMeans(X)))
obs <- t(apply(obs, MARGIN=1, function(x) x/apply(X , MARGIN=2, FUN=sd)))
O <- matrix(unlist(rep(unname(obs),N)),byrow=T,nrow=N)  # obs values as matrix
Y <- as.matrix(df[,response]) # Response variable as matrix
dimnames(Y)[[2]] <- list(response)
if(PCA){
red_dim <- prcomp(X, scale = T, rank=2); p <- 2
rot_var <- red_dim$rotation[,c(1,2)]
X <- red_dim$x[,c(1,2)]
O <- O %*% rot_var
}
dimnames(O)[[1]] = dimnames(Y)[[1]] <- dimnames(X)[[1]]  # Keeping same sample names
dimnames(O)[[2]] <- dimnames(X)[[2]] # Keeping same feature names
D <- matrix((X-O)^2 %*% matrix(rep(1, p), ncol = 1))  # Euclidean distance calculation
D <- order(D,decreasing = F)[1:k]  # Filtering k-nearest neighbours
if(is.numeric(df[,response])){  # regression mode
obs[,response] <- mean(Y[D,])
return(obs)
} else{  # classification mode
votes <- table(Y[D])
# picking randomly a choice if equal number of choices
obs[,response] <- names(votes[votes==max(votes)])
prop_votes <- votes[obs[,response]]/sum(votes)
return(list(class=obs, confidence=prop_votes))
}
}
pred_class <- list(class=rep(NA,nrow(iris)), conf=rep(NA,nrow(iris)))
for(i in 1:nrow(iris)){
pred_class$class[i] <- knn(df = iris[-i,], k = 4, obs = iris[i,], response = 'Species')$class[,'Species']
pred_class$conf[i] <- knn(df = iris[-i,], k = 10, obs = iris[i,], response = 'Species')$confidence
}
N <- nrow(df)  # Number of observations
p <- length(num_features)  # Number of input variables
X <- as.matrix(df[,num_features])  # Transforming into matrix
obs <- t(apply(obs, MARGIN=1, function(x) x-colMeans(X)))
obs <- t(apply(obs, MARGIN=1, function(x) x/apply(X , MARGIN=2, FUN=sd)))
O <- matrix(unlist(rep(unname(obs),N)),byrow=T,nrow=N)  # obs values as matrix
Y <- as.matrix(df[,response]) # Response variable as matrix
dimnames(Y)[[2]] <- list(response)
if(PCA){
red_dim <- prcomp(X, scale = T, rank=2); p <- 2
rot_var <- red_dim$rotation[,c(1,2)]
X <- red_dim$x[,c(1,2)]
O <- O %*% rot_var
}
dimnames(O)[[1]] = dimnames(Y)[[1]] <- dimnames(X)[[1]]  # Keeping same sample names
dimnames(O)[[2]] <- dimnames(X)[[2]] # Keeping same feature names
red_dim <- prcomp(X, scale = T, rank=2); p <- 2
rot_var <- red_dim$rotation[,c(1,2)]
X <- red_dim$x[,c(1,2)]
O <- O %*% rot_var
dimnames(O)[[1]] = dimnames(Y)[[1]] <- dimnames(X)[[1]]  # Keeping same sample names
dimnames(O)[[2]] <- dimnames(X)[[2]] # Keeping same feature names
D <- matrix((X-O)^2 %*% matrix(rep(1, p), ncol = 1))  # Euclidean distance calculation
D <- order(D,decreasing = F)[1:k]  # Filtering k-nearest neighbours
votes <- table(Y[D])
obs[,response] <- names(votes[votes==max(votes)])
table(Y[D])
names(votes[votes==max(votes)])
obs
obs[,response]
obs[,response] <- names(votes[votes==max(votes)])
response
obs <- select(obs,-matches(response))
knn <- function(df, k, PCA=T, obs, response){
# implementing k-nearest neighbours. This function can work with
# categorical or numeric response. It can only predict one observation
# at a time.
orig <- obs
obs <- select(obs,-matches(response))
isNum <- sapply(obs,is.numeric)
obs <- select(obs, which(isNum))
num_features <- colnames(df)[unname(sapply(df,is.numeric))]  # Numeric variables only
num_features <- num_features[num_features != response]
N <- nrow(df)  # Number of observations
p <- length(num_features)  # Number of input variables
X <- as.matrix(df[,num_features])  # Transforming into matrix
obs <- t(apply(obs, MARGIN=1, function(x) x-colMeans(X)))
obs <- t(apply(obs, MARGIN=1, function(x) x/apply(X , MARGIN=2, FUN=sd)))
O <- matrix(unlist(rep(unname(obs),N)),byrow=T,nrow=N)  # obs values as matrix
Y <- as.matrix(df[,response]) # Response variable as matrix
dimnames(Y)[[2]] <- list(response)
if(PCA){
red_dim <- prcomp(X, scale = T, rank=2); p <- 2
rot_var <- red_dim$rotation[,c(1,2)]
X <- red_dim$x[,c(1,2)]
O <- O %*% rot_var
}
dimnames(O)[[1]] = dimnames(Y)[[1]] <- dimnames(X)[[1]]  # Keeping same sample names
dimnames(O)[[2]] <- dimnames(X)[[2]] # Keeping same feature names
D <- matrix((X-O)^2 %*% matrix(rep(1, p), ncol = 1))  # Euclidean distance calculation
D <- order(D,decreasing = F)[1:k]  # Filtering k-nearest neighbours
if(is.numeric(df[,response])){  # regression mode
obs[,response] <- mean(Y[D,])
return(obs)
} else{  # classification mode
votes <- table(Y[D])
# picking randomly a choice if equal number of choices
orig[,response] <- names(votes[votes==max(votes)])
prop_votes <- votes[obs[,response]]/sum(votes)
return(list(class=orig, confidence=prop_votes))
}
}
knn(df = swis
)
pred_class <- list(class=rep(NA,nrow(iris)), conf=rep(NA,nrow(iris)))
for(i in 1:nrow(iris)){
pred_class$class[i] <- knn(df = iris[-i,], k = 4, obs = iris[i,], response = 'Species')$class[,'Species']
pred_class$conf[i] <- knn(df = iris[-i,], k = 10, obs = iris[i,], response = 'Species')$confidence
}
swiss
pred_class <- list(class=rep(NA,nrow(iris)), conf=rep(NA,nrow(iris)))
for(i in 1:nrow(iris)){
pred_class$class[i] <- knn(df = iris[-i,], k = 4, obs = iris[i,], response = 'Species')$class[,'Species']
pred_class$conf[i] <- knn(df = iris[-i,], k = 10, obs = iris[i,], response = 'Species')$confidence
}
obs
names(votes[votes==max(votes)])
obs[,response] <- names(votes[votes==max(votes)])
obs[response] <- names(votes[votes==max(votes)])
obs
knn <- function(df, k, PCA=T, obs, response){
# implementing k-nearest neighbours. This function can work with
# categorical or numeric response. It can only predict one observation
# at a time.
obs <- select(obs,-matches(response))
isNum <- sapply(obs,is.numeric)
obs <- select(obs, which(isNum))
num_features <- colnames(df)[unname(sapply(df,is.numeric))]  # Numeric variables only
num_features <- num_features[num_features != response]
N <- nrow(df)  # Number of observations
p <- length(num_features)  # Number of input variables
X <- as.matrix(df[,num_features])  # Transforming into matrix
obs <- t(apply(obs, MARGIN=1, function(x) x-colMeans(X)))
obs <- t(apply(obs, MARGIN=1, function(x) x/apply(X , MARGIN=2, FUN=sd)))
O <- matrix(unlist(rep(unname(obs),N)),byrow=T,nrow=N)  # obs values as matrix
Y <- as.matrix(df[,response]) # Response variable as matrix
dimnames(Y)[[2]] <- list(response)
if(PCA){
red_dim <- prcomp(X, scale = T, rank=2); p <- 2
rot_var <- red_dim$rotation[,c(1,2)]
X <- red_dim$x[,c(1,2)]
O <- O %*% rot_var
}
dimnames(O)[[1]] = dimnames(Y)[[1]] <- dimnames(X)[[1]]  # Keeping same sample names
dimnames(O)[[2]] <- dimnames(X)[[2]] # Keeping same feature names
D <- matrix((X-O)^2 %*% matrix(rep(1, p), ncol = 1))  # Euclidean distance calculation
D <- order(D,decreasing = F)[1:k]  # Filtering k-nearest neighbours
if(is.numeric(df[,response])){  # regression mode
pred_val <- mean(Y[D,])
prop_votes=NA
return(obs)
} else{  # classification mode
votes <- table(Y[D])
# picking randomly a choice if equal number of choices
pred_val <- names(votes[votes==max(votes)])
prop_votes <- votes[obs[,response]]/sum(votes)
return(list(prediction=pred_val, confidence=prop_votes))
}
}
pred_class <- list(class=rep(NA,nrow(iris)), conf=rep(NA,nrow(iris)))
for(i in 1:nrow(iris)){
pred_class$class[i] <- knn(df = iris[-i,], k = 4, obs = iris[i,], response = 'Species')$class[,'Species']
pred_class$conf[i] <- knn(df = iris[-i,], k = 10, obs = iris[i,], response = 'Species')$confidence
}
knn(df = iris[-i,], k = 4, obs = iris[i,], response = 'Species')
knn(df = iris[-i,], k = 4, obs = iris[i,], response = 'Species')$prediction
knn <- function(df, k, PCA=T, obs, response){
# implementing k-nearest neighbours. This function can work with
# categorical or numeric response. It can only predict one observation
# at a time.
obs <- select(obs,-matches(response))
isNum <- sapply(obs,is.numeric)
obs <- select(obs, which(isNum))
num_features <- colnames(df)[unname(sapply(df,is.numeric))]  # Numeric variables only
num_features <- num_features[num_features != response]
N <- nrow(df)  # Number of observations
p <- length(num_features)  # Number of input variables
X <- as.matrix(df[,num_features])  # Transforming into matrix
obs <- t(apply(obs, MARGIN=1, function(x) x-colMeans(X)))
obs <- t(apply(obs, MARGIN=1, function(x) x/apply(X , MARGIN=2, FUN=sd)))
O <- matrix(unlist(rep(unname(obs),N)),byrow=T,nrow=N)  # obs values as matrix
Y <- as.matrix(df[,response]) # Response variable as matrix
dimnames(Y)[[2]] <- list(response)
if(PCA){
red_dim <- prcomp(X, scale = T, rank=2); p <- 2
rot_var <- red_dim$rotation[,c(1,2)]
X <- red_dim$x[,c(1,2)]
O <- O %*% rot_var
}
dimnames(O)[[1]] = dimnames(Y)[[1]] <- dimnames(X)[[1]]  # Keeping same sample names
dimnames(O)[[2]] <- dimnames(X)[[2]] # Keeping same feature names
D <- matrix((X-O)^2 %*% matrix(rep(1, p), ncol = 1))  # Euclidean distance calculation
D <- order(D,decreasing = F)[1:k]  # Filtering k-nearest neighbours
if(is.numeric(df[,response])){  # regression mode
pred_val <- mean(Y[D,])
return(pred_val)
} else{  # classification mode
votes <- table(Y[D])
# picking randomly a choice if equal number of choices
pred_val <- names(votes[votes==max(votes)])
prop_votes <- votes[pred_val]/sum(votes)
return(list(prediction=pred_val, confidence=prop_votes))
}
}
pred_class <- list(class=rep(NA,nrow(iris)), conf=rep(NA,nrow(iris)))
for(i in 1:nrow(iris)){
pred_class$class[i] <- knn(df = iris[-i,], k = 4, obs = iris[i,], response = 'Species')$prediction
pred_class$conf[i] <- knn(df = iris[-i,], k = 10, obs = iris[i,], response = 'Species')$confidence
}
warnings()
knn(df = iris[-i,], k = 4, obs = iris[i,], response = 'Species')$prediction
names(votes[votes==max(votes)])
pred_class <- list(class=rep(NA,nrow(iris)), conf=rep(NA,nrow(iris)))
for(i in 1:nrow(iris)){
pred_class$class[i] <- knn(df = iris[-i,], k = 10, obs = iris[i,], response = 'Species')$prediction
pred_class$conf[i] <- knn(df = iris[-i,], k = 10, obs = iris[i,], response = 'Species')$confidence
}
pred_class <- list(class=rep(NA,nrow(iris)), conf=rep(NA,nrow(iris)))
for(i in 1:nrow(iris)){
pred_class$class[i] <- knn(df = iris[-i,], k = 100, obs = iris[i,], response = 'Species')$prediction
pred_class$conf[i] <- knn(df = iris[-i,], k = 100, obs = iris[i,], response = 'Species')$confidence
}
pred_class <- list(class=rep(NA,nrow(iris)), conf=rep(NA,nrow(iris)))
for(i in 1:nrow(iris)){
pred_class$class[i] <- knn(df = iris[-i,], k = 100, obs = iris[i,], response = 'Species')$prediction
pred_class$conf[i] <- knn(df = iris[-i,], k = 100, obs = iris[i,], response = 'Species')$confidence
}
pred_class
iris[iris$Species!=pred_class$class]
iris[iris$Species!=pred_class$class,]
pred_class <- list(class=rep(NA,nrow(iris)), conf=rep(NA,nrow(iris)))
for(i in 1:nrow(iris)){
pred_class$class[i] <- knn(df = iris[-i,], k = 10, obs = iris[i,], response = 'Species')$prediction
pred_class$conf[i] <- knn(df = iris[-i,], k = 10, obs = iris[i,], response = 'Species')$confidence
}
iris[iris$Species!=pred_class$class,]
pred_class <- list(class=rep(NA,nrow(iris)), conf=rep(NA,nrow(iris)))
for(i in 1:nrow(iris)){
pred_class$class[i] <- knn(df = iris[-i,], k = 5, obs = iris[i,], response = 'Species')$prediction
pred_class$conf[i] <- knn(df = iris[-i,], k = 5, obs = iris[i,], response = 'Species')$confidence
}
iris[iris$Species!=pred_class$class,]
knn <- function(df, k, PCA=T, obs, response){
# implementing k-nearest neighbours. This function can work with
# categorical or numeric response. It can only predict one observation
# at a time.
obs <- select(obs,-matches(response))
isNum <- sapply(obs,is.numeric)
obs <- select(obs, which(isNum))
num_features <- colnames(df)[unname(sapply(df,is.numeric))]  # Numeric variables only
num_features <- num_features[num_features != response]
N <- nrow(df)  # Number of observations
p <- length(num_features)  # Number of input variables
X <- as.matrix(df[,num_features])  # Transforming into matrix
obs <- t(apply(obs, MARGIN=1, function(x) x-colMeans(X)))
obs <- t(apply(obs, MARGIN=1, function(x) x/apply(X , MARGIN=2, FUN=sd)))
O <- matrix(unlist(rep(unname(obs),N)),byrow=T,nrow=N)  # obs values as matrix
Y <- as.matrix(df[,response]) # Response variable as matrix
dimnames(Y)[[2]] <- list(response)
if(PCA){
red_dim <- prcomp(X, scale = T, rank=2); p <- 2
rot_var <- red_dim$rotation[,c(1,2)]
X <- red_dim$x[,c(1,2)]
O <- O %*% rot_var
}
dimnames(O)[[1]] = dimnames(Y)[[1]] <- dimnames(X)[[1]]  # Keeping same sample names
dimnames(O)[[2]] <- dimnames(X)[[2]] # Keeping same feature names
D <- matrix((X-O)^2 %*% matrix(rep(1, p), ncol = 1))  # Euclidean distance calculation
D <- order(D,decreasing = F)[1:k]  # Filtering k-nearest neighbours
if(is.numeric(df[,response])){  # regression mode
pred_val <- mean(Y[D,])
return(pred_val)
} else{  # classification mode
votes <- table(Y[D])
# picking randomly a choice if equal number of choices
pred_val <- names(votes[votes==max(votes)])
prop_votes <- votes[pred_val]/sum(votes)
return(list(prediction=pred_val, confidence=prop_votes))
}
}
knn(df = swiss[-1,], k = 5, obs = swiss[1,], response = 'Infant.Mortality')
knn(df = iris[-1,], k = 10, obs = iris[100,], response = 'Petal.Width')
pred_class <- list(class=rep(NA,nrow(iris)), conf=rep(NA,nrow(iris)))
for(i in 1:nrow(iris)){
pred_class$class[i] <- knn(df = iris[-i,], k = 7, obs = iris[i,], response = 'Species')$prediction
pred_class$conf[i] <- knn(df = iris[-i,], k = 7, obs = iris[i,], response = 'Species')$confidence
}
pred_class
iris[iris$Species!=pred_class$class,]
nrow(iris[iris$Species!=pred_class$class,])
nrow(iris[iris$Species!=pred_class$class,])/15''
nrow(iris[iris$Species!=pred_class$class,])/150
head(swiss)
iris[iris$Species!=pred_class$class,]
plot(prcomp(iris[,-5], scale.=T, rank=2)$x)
plot(prcomp(iris[,-5], scale.=T, rank=2)$x, col=rownames(iris[iris$Species!=pred_class]))
plot(prcomp(iris[,-5], scale.=T, rank=2)$x, col=rownames(iris[iris$Species!=pred_class,]))
iris[iris$Species!=pred_class,]
iris[iris$Species!=pred_class$class,]
plot(prcomp(iris[,-5], scale.=T, rank=2)$x, col=rownames(iris[iris$Species!=pred_class$class,]))
rownames(iris[iris$Species!=pred_class$class,])
plot(prcomp(iris[,-5], scale.=T, rank=2)$x)
iris[iris$Species!=pred_class$class,]
points(prcomp(iris[,-5], scale.=T, rank=2)$x[iris[iris$Species!=pred_class$class,],], col=red))
points(prcomp(iris[,-5], scale.=T, rank=2)$x[iris[iris$Species!=pred_class$class,],], col="red"))
points(prcomp(iris[,-5], scale.=T, rank=2)$x[iris[iris$Species!=pred_class$class,],], col="red")
points(prcomp(iris[,-5], scale.=T, rank=2)$x[iris$Species!=pred_class$class,], col="red")
points(prcomp(iris[,-5], scale.=T, rank=2)$x[iris$Species!=pred_class$class,], col="red", pch=3)
plot(prcomp(iris[,-5], scale.=T, rank=2)$x, col=rownames(iris[iris$Species!=pred_class$class,]))
plot(prcomp(iris[,-5], scale.=T, rank=2)$x, col=iris$Species)
points(prcomp(iris[,-5], scale.=T, rank=2)$x[iris$Species!=pred_class$class,], col="red", pch=3)
