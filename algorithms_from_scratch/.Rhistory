Sys.sleep(4)
return(list(class=obs, confidence=prop_votes[names(out_vote)]))
}
}
knn(df = iris[-1,], k = 10, obs = iris[1,], softmax = F, response = 'Petal.Width')
knn <- function(df, k, softmax=F, obs, response){
# implementing k-nearest neighbours. This function can work with
# categorical or numeric response. It can only predict one observation
# at a time. Possibility to chose between softmax weighted votes or simple
# cut-off at k-nearest neighbours.
obs <- select(obs,-matches(response))
isNum <- sapply(obs,is.numeric)
obs <- select(obs, which(isNum))
num_features <- colnames(df)[unname(sapply(df,is.numeric))]  # Numeric variables only
num_features <- num_features[num_features != response]
N <- nrow(df)  # Number of observations
p <- length(num_features)  # Number of input variables
X <- as.matrix(df[,num_features])  # Transforming into matrix
Y <- as.matrix(df[,response]) # Response variable as matrix
dimnames(Y)[[2]] <- list(response)
O <- matrix(unlist(rep(unname(obs),N)),byrow=T,nrow=N)  # obs values as matrix
dimnames(O)[[1]] = dimnames(Y)[[1]] <- dimnames(X)[[1]]  # Keeping same sample names
dimnames(O)[[2]] <- dimnames(X)[[2]] # Keeping same feature names
Sys.sleep(4)
D <- (X-O)^2 %*% matrix(rep(1, p), ncol = 1)  # Euclidean distance calculation
D <- D[order(D[,1]),][1:k]  # Filtering k-nearest neighbours
if(softmax){D <- 1/(1+exp(-(D-mean(D))/sd(D)))} # Normalize distance to reduce impact of outliers
if(is.numeric(df[,response])){  # regression mode
weight <- ifelse(softmax,yes = list(round(exp(-D)/sum(exp(-D)),3)),
no = list(rep(x = 1/k,times=k)))[[1]]
# weights are either all the same or defined by softmax function
obs[,response] <- weight %*% as.matrix(Y[names(D),])
return(obs)
} else{  # classification mode
votes <- table(namesD)
if(softmax){
out_vote <- votes[votes==max(votes)]
out_vote <- out_vote[sample(x = 1:length(out_vote),size = 1)]
prop_votes <- sapply(votes,function(x) x/sum(votes))
}
# picking randomly a choice if equal number of choices
obs[,response] <- out_vote
return(list(class=obs, confidence=prop_votes[names(out_vote)]))
}
}
knn(df = iris[-1,], k = 10, obs = iris[1,], softmax = F, response = 'Petal.Width')
knn(df = iris[-1,], k = 10, obs = iris[1,], softmax = F, response = 'Petal.Width')
knn <- function(df, k, softmax=F, obs, response){
# implementing k-nearest neighbours. This function can work with
# categorical or numeric response. It can only predict one observation
# at a time. Possibility to chose between softmax weighted votes or simple
# cut-off at k-nearest neighbours.
obs <- select(obs,-matches(response))
isNum <- sapply(obs,is.numeric)
obs <- select(obs, which(isNum))
num_features <- colnames(df)[unname(sapply(df,is.numeric))]  # Numeric variables only
num_features <- num_features[num_features != response]
N <- nrow(df)  # Number of observations
p <- length(num_features)  # Number of input variables
X <- as.matrix(df[,num_features])  # Transforming into matrix
Y <- as.matrix(df[,response]) # Response variable as matrix
dimnames(Y)[[2]] <- list(response)
O <- matrix(unlist(rep(unname(obs),N)),byrow=T,nrow=N)  # obs values as matrix
dimnames(O)[[1]] = dimnames(Y)[[1]] <- dimnames(X)[[1]]  # Keeping same sample names
dimnames(O)[[2]] <- dimnames(X)[[2]] # Keeping same feature names
D <- (X-O)^2 %*% matrix(rep(1, p), ncol = 1)  # Euclidean distance calculation
D <- D[order(D[,1]),][1:k]  # Filtering k-nearest neighbours
if(softmax){D <- 1/(1+exp(-(D-mean(D))/sd(D)))} # Normalize distance to reduce impact of outliers
if(is.numeric(df[,response])){  # regression mode
weight <- ifelse(softmax,yes = list(round(exp(-D)/sum(exp(-D)),3)),
no = list(rep(x = 1/k,times=k)))[[1]]
# weights are either all the same or defined by softmax function
obs[,response] <- weight %*% as.matrix(Y[names(D),])
return(obs)
} else{  # classification mode
votes <- table(namesD)
if(softmax){
out_vote <- votes[votes==max(votes)]
out_vote <- out_vote[sample(x = 1:length(out_vote),size = 1)]
prop_votes <- sapply(votes,function(x) x/sum(votes))
}
# picking randomly a choice if equal number of choices
obs[,response] <- out_vote
return(list(class=obs, confidence=prop_votes[names(out_vote)]))
}
}
knn(df = iris[-1,], k = 10, obs = iris[1,], softmax = F, response = 'Petal.Width')
train_mod <- function(df, response, exact=T,speed=0.6, iter=200, plot=F){
# Takes a dataframe and the name of the variable to predict as input.
# Returns the input dataframe with the estimated age after minimizing RSS
num_features <- colnames(df)[unname(sapply(df,is.numeric))]  # Numeric variables only
num_features <- num_features[num_features != response]  # Excluding response variable
N <- nrow(df)  # Number of observations
p <- length(num_features)  # Number of input variables
X <- as.matrix(df[,num_features])  # Transforming into matrix
X <- cbind(rep(1,N),X)  # Adding intercept (bias)
df[,response] <- as.numeric(df[,response])  # If response is factorial, encoded with integers
Y <- as.matrix(df[,response])  # Response variable as matrix
# Plotting input-response correlations
if(plot==T){
par(mfrow=c(floor(sqrt(p)),p/floor(sqrt(p))))
for(var in num_features){plot(df[,var],df[,response],main=var)}
}
if(exact==T){
# Solution of the derivative by beta to minimize RSS(B)
B_hat <- solve(t(X)%*%X) %*% t(X) %*% Y
} else{
# Using gradient descent method. Approximate solution, but
# faster in very high dimensional space with many observations
B_hat <- matrix(rep(1,p+1),ncol=1)  # Initiating weights
for(i in 1:iter){
Y_hat <- X %*% B_hat  # Computing estimated response from weights
gradients <- (2/N) * (t((Y_hat - Y)) %*% X)
# Using partial derivative of each weight to compute gradient
B_hat <- B_hat - t(gradients) * speed  # Updating weights using gradients
}
}
Y_hat <- X %*% B_hat
df_out <- cbind(df, Y_hat)
return(list(table=df_out,weights=B_hat))
}
predict_mod <- function(obs, response, weights){
# Predicting observations using pre-calculated weights
num_features <- colnames(obs)[unname(sapply(obs,is.numeric))]  # Numeric variables only
num_features <- num_features[num_features != response]  # Excluding response variable
N <- nrow(obs)  # Number of observations
p <- length(num_features)  # Number of input variables
X <- as.matrix(obs[,num_features])  # Transforming into matrix
X <- cbind(rep(1,N),X)  # Adding intercept (bias)
predicted <- X %*% weights  # Estimating response variable
return(predicted)
}
cross_val <- function(data, response, folds, exact=T, speed=0.001, iter=200){
# Cross validation: leave one out
init <- rep(NA,nrow(data))  #  initializing df for cross-val. output
accuracy <- data.frame(pred=init, folds=init)
#accuracy <- data.frame(pred = numeric(0), folds = numeric(0))
weight_list <- list(); MSE=rep(NA, folds)
pool <- 1:nrow(data)  # pool of rows from which to sample folds
fold_size <- floor(nrow(data)/folds)  # Number of samples per fold
fold_list <- matrix(ncol = folds, nrow = fold_size)  # initializing data structure for folds
for(i in 1:folds){  # Leave one out (each obs once)
fold_list[,i] <- sample(pool[!is.na(pool)], size = fold_size, replace = F)  # randomly sampling fold
pool[fold_list[,i]] <- NA #  removing samples in fold from sample pool
trained <- train_mod(data[-fold_list[,i],], response, exact, speed, iter)  # Training without fold i
test_obs <- predict_mod(data[fold_list[,i],], response, trained$weights)  # Predicting fold i
SE <- (as.numeric(test_obs) - as.numeric(data[fold_list[,i],response]))^2 # Squared error
MSE[i] <- mean(SE)
# computing mean squared error from real value
weight_list[[i]] <- trained$weights
test_obs <- data.frame(pred=test_obs,folds=rep(i,fold_size))
accuracy[fold_list[,i],] <- test_obs
#accuracy <- rbind(accuracy, test_obs)
Sys.sleep(0.5)
}
return(list(accuracy=accuracy, weights=weight_list, MSE=MSE))
}
out_var='Petal.Length'
cross_val(iris, out_var, folds=150, exact=T)
for(it in seq(1,3000,10)){
results <-cross_val(iris,out_var, folds=3, exact=F, speed=0.0005, iter = it)
plot(as.numeric(iris[,out_var]),results$accuracy$pred,col=results$accuracy$folds,
xlim=c(0,10),ylim=c(0,10))
abline(a=0,b=1)
Sys.sleep(0.2)
}
for(it in seq(1,3000,10)){
results <-cross_val(iris,out_var, folds=3, exact=F, speed=0.0005, iter = it)
plot(as.numeric(iris[,out_var]),results$accuracy$pred,col=results$accuracy$folds,
xlim=c(0,10),ylim=c(0,10))
abline(a=0,b=1)
Sys.sleep(0.2)
}
library(tidyverse)
knn <- function(df, k, softmax=F, obs, response){
# implementing k-nearest neighbours. This function can work with
# categorical or numeric response. It can only predict one observation
# at a time. Possibility to chose between softmax weighted votes or simple
# cut-off at k-nearest neighbours.
obs <- select(obs,-matches(response))
isNum <- sapply(obs,is.numeric)
obs <- select(obs, which(isNum))
num_features <- colnames(df)[unname(sapply(df,is.numeric))]  # Numeric variables only
num_features <- num_features[num_features != response]
N <- nrow(df)  # Number of observations
p <- length(num_features)  # Number of input variables
X <- as.matrix(df[,num_features])  # Transforming into matrix
Y <- as.matrix(df[,response]) # Response variable as matrix
dimnames(Y)[[2]] <- list(response)
O <- matrix(unlist(rep(unname(obs),N)),byrow=T,nrow=N)  # obs values as matrix
dimnames(O)[[1]] = dimnames(Y)[[1]] <- dimnames(X)[[1]]  # Keeping same sample names
dimnames(O)[[2]] <- dimnames(X)[[2]] # Keeping same feature names
D <- (X-O)^2 %*% matrix(rep(1, p), ncol = 1)  # Euclidean distance calculation
D <- D[order(D[,1]),][1:k]  # Filtering k-nearest neighbours
if(softmax){D <- 1/(1+exp(-(D-mean(D))/sd(D)))} # Normalize distance to reduce impact of outliers
if(is.numeric(df[,response])){  # regression mode
weight <- ifelse(softmax,yes = list(round(exp(-D)/sum(exp(-D)),3)),
no = list(rep(x = 1/k,times=k)))[[1]]
# weights are either all the same or defined by softmax function
obs[,response] <- weight %*% as.matrix(Y[names(D),])
return(obs)
} else{  # classification mode
votes <- table(namesD)
if(softmax){
out_vote <- votes[votes==max(votes)]
out_vote <- out_vote[sample(x = 1:length(out_vote),size = 1)]
prop_votes <- sapply(votes,function(x) x/sum(votes))
}
# picking randomly a choice if equal number of choices
obs[,response] <- out_vote
return(list(class=obs, confidence=prop_votes[names(out_vote)]))
}
}
knn(df = swiss[-1,], k = 5, obs = swiss[1,], softmax = F, response = 'Infant.Mortality')
library(tidyverse)
knn <- function(df, k, obs, response){
# implementing k-nearest neighbours. This function can work with
# categorical or numeric response. It can only predict one observation
# at a time.
obs <- select(obs,-matches(response))
isNum <- sapply(obs,is.numeric)
obs <- select(obs, which(isNum))
num_features <- colnames(df)[unname(sapply(df,is.numeric))]  # Numeric variables only
num_features <- num_features[num_features != response]
N <- nrow(df)  # Number of observations
p <- length(num_features)  # Number of input variables
X <- as.matrix(df[,num_features])  # Transforming into matrix
Y <- as.matrix(df[,response]) # Response variable as matrix
dimnames(Y)[[2]] <- list(response)
O <- matrix(unlist(rep(unname(obs),N)),byrow=T,nrow=N)  # obs values as matrix
dimnames(O)[[1]] = dimnames(Y)[[1]] <- dimnames(X)[[1]]  # Keeping same sample names
dimnames(O)[[2]] <- dimnames(X)[[2]] # Keeping same feature names
D <- (X-O)^2 %*% matrix(rep(1, p), ncol = 1)  # Euclidean distance calculation
D <- D[order(D[,1]),][1:k]  # Filtering k-nearest neighbours
if(is.numeric(df[,response])){  # regression mode
weight <- rep(x = 1/k,times=k)
# weights are either all the same or defined by softmax function
obs[,response] <- mean(Y[names(D),])
return(obs)
} else{  # classification mode
votes <- table(namesD)
# picking randomly a choice if equal number of choices
obs[,response] <- votes[votes==max(votes)]
return(list(class=obs, confidence=prop_votes[names(out_vote)]))
}
}
knn(df = swiss[-1,], k = 5, obs = swiss[1,], softmax = F, response = 'Infant.Mortality')
knn(df = swiss[-1,], k = 5, obs = swiss[1,], response = 'Infant.Mortality')
knn(df = iris[-1,], k = 10, obs = iris[1,], response = 'Petal.Width')
iris[1,]
knn(df = iris[-1,], k = 10, obs = iris[100,], response = 'Petal.Width')
iris[100,]
prcomp(iris)
prcomp(x = iris)
prcomp(x = iris[,c("Sepal.Width","Petal.Length")])
prcomp(x = iris[,c("Sepal.Width","Petal.Length", "Species")])
select(iris, -matches("Species"))
prcomp(x = select(iris, -matches("Species")))
prcomp(x = t(iris))
prcomp(x = t(select(iris, -matches("Species"))))
PC_iris <- prcomp(x = t(select(iris, -matches("Species"))))
PC_iris$scale
PC_iris$sdev
PC_iris$rotation[,"PC1"]
plot(PC_iris$rotation[,"PC1"], PC_iris$rotation[,"PC2"],col=iris$Species)
plot(PC_iris$rotation[,"PC1"], PC_iris$rotation[,"PC3"],col=iris$Species)
plot(PC_iris$rotation[,"PC1"], PC_iris$rotation[,"PC4"],col=iris$Species)
plot(PC_iris$rotation[,"PC1"], PC_iris$rotation[,"PC2"],col=iris$Species)
plot(PC_iris$rotation[,"PC1"], PC_iris$rotation[,"PC2"],
col=iris$Species, scaled=T)
plot(PC_iris$rotation[,"PC1"], PC_iris$rotation[,"PC2"],
col=iris$Species, scaled=T)
plot(PC_iris$rotation[,"PC1"], PC_iris$rotation[,"PC2"],
col=iris$Species, scaled=T)
warnings()
plot(PC_iris$rotation[,"PC1"], PC_iris$rotation[,"PC2"],
col=iris$Species, scale=T)
PC_iris <- prcomp(x = t(select(iris, -matches("Species"))),scale. = T)
plot(PC_iris$rotation[,"PC1"], PC_iris$rotation[,"PC2"],
col=iris$Species)
plot(PC_iris$rotation[,"PC1"], PC_iris$rotation[,"PC3"],
col=iris$Species)
plot(PC_iris$rotation[,"PC4"], PC_iris$rotation[,"PC3"],
col=iris$Species)
PC_iris <- prcomp(x = t(select(iris, -matches("Species"))),scale. = T)
plot(PC_iris$rotation[,"PC1"], PC_iris$rotation[,"PC2"],
plot(PC_iris$rotation[,"PC1"], PC_iris$rotation[,"PC2"],
col=iris$Species)
)
plot(PC_iris$rotation[,"PC1"], PC_iris$rotation[,"PC2"],
col=iris$Species)
plot((PC_iris$rotation[,"PC1"])^2, PC_iris$rotation[,"PC2"],
col=iris$Species)
plot((PC_iris$rotation[,"PC1"])^6, PC_iris$rotation[,"PC2"],
col=iris$Species)
plot((PC_iris$rotation[,"PC1"])^60, PC_iris$rotation[,"PC2"],
col=iris$Species)
plot(sqrt(PC_iris$rotation[,"PC1"]), PC_iris$rotation[,"PC2"],
col=iris$Species)
plot(log(PC_iris$rotation[,"PC1"]), PC_iris$rotation[,"PC2"],
col=iris$Species)
plot(abs(PC_iris$rotation[,"PC1"]), PC_iris$rotation[,"PC2"],
col=iris$Species)
plot(log(abs(PC_iris$rotation[,"PC1"])), PC_iris$rotation[,"PC2"],
col=iris$Species)
plot(sin(abs(PC_iris$rotation[,"PC1"])), PC_iris$rotation[,"PC2"],
col=iris$Species)
plot(cos(abs(PC_iris$rotation[,"PC1"])), PC_iris$rotation[,"PC2"],
col=iris$Species)
plot(abs(PC_iris$rotation[,"PC1"]), PC_iris$rotation[,"PC2"],
col=iris$Species)
swiss
PC_iris <- prcomp(x = t(select(iris, -matches("Species"))))
plot(abs(PC_iris$rotation[,"PC1"]), PC_iris$rotation[,"PC2"],
col=iris$Species)
pairs(PC_iris)
pairs(PC_iris$rotation)
pairs(PC_iris$rotation,col=iris$Species)
pairs(iris[,-'Species'],col=iris$Species)
pairs(iris[,-c('Species')],col=iris$Species)
pairs(select(iris,-matches('Species')],col=iris$Species)
conf_mat <- function(){
# confusion matrix to assess quality of model
}
pairs(select(iris,-matches('Species')),col=iris$Species)
knn <- function(df, k, obs, response){
# implementing k-nearest neighbours. This function can work with
# categorical or numeric response. It can only predict one observation
# at a time.
obs <- select(obs,-matches(response))
isNum <- sapply(obs,is.numeric)
obs <- select(obs, which(isNum))
num_features <- colnames(df)[unname(sapply(df,is.numeric))]  # Numeric variables only
num_features <- num_features[num_features != response]
N <- nrow(df)  # Number of observations
p <- length(num_features)  # Number of input variables
X <- as.matrix(df[,num_features])  # Transforming into matrix
Y <- as.matrix(df[,response]) # Response variable as matrix
dimnames(Y)[[2]] <- list(response)
O <- matrix(unlist(rep(unname(obs),N)),byrow=T,nrow=N)  # obs values as matrix
dimnames(O)[[1]] = dimnames(Y)[[1]] <- dimnames(X)[[1]]  # Keeping same sample names
dimnames(O)[[2]] <- dimnames(X)[[2]] # Keeping same feature names
D <- (X-O)^2 %*% matrix(rep(1, p), ncol = 1)  # Euclidean distance calculation
D <- D[order(D[,1]),][1:k]  # Filtering k-nearest neighbours
if(is.numeric(df[,response])){  # regression mode
weight <- rep(x = 1/k,times=k)
# weights are either all the same or defined by softmax function
obs[,response] <- mean(Y[names(D),])
return(obs)
} else{  # classification mode
votes <- table(namesD)
# picking randomly a choice if equal number of choices
obs[,response] <- votes[votes==max(votes)]
return(list(class=obs, confidence=prop_votes[names(out_vote)]))
}
}
knn(df = swiss[-1,], k = 5, obs = swiss[1,], response = 'Infant.Mortality')
knn(df = iris[-1,], k = 10, obs = iris[100,], response = 'Petal.Width')
knn <- function(df, k, obs, response){
# implementing k-nearest neighbours. This function can work with
# categorical or numeric response. It can only predict one observation
# at a time.
obs <- select(obs,-matches(response))
isNum <- sapply(obs,is.numeric)
obs <- select(obs, which(isNum))
num_features <- colnames(df)[unname(sapply(df,is.numeric))]  # Numeric variables only
num_features <- num_features[num_features != response]
N <- nrow(df)  # Number of observations
p <- length(num_features)  # Number of input variables
X <- as.matrix(df[,num_features])  # Transforming into matrix
X <- prcomp(t(X))$rotation
Y <- as.matrix(df[,response]) # Response variable as matrix
dimnames(Y)[[2]] <- list(response)
O <- matrix(unlist(rep(unname(obs),N)),byrow=T,nrow=N)  # obs values as matrix
dimnames(O)[[1]] = dimnames(Y)[[1]] <- dimnames(X)[[1]]  # Keeping same sample names
dimnames(O)[[2]] <- dimnames(X)[[2]] # Keeping same feature names
D <- (X-O)^2 %*% matrix(rep(1, p), ncol = 1)  # Euclidean distance calculation
D <- D[order(D[,1]),][1:k]  # Filtering k-nearest neighbours
if(is.numeric(df[,response])){  # regression mode
obs[,response] <- mean(Y[names(D),])
return(obs)
} else{  # classification mode
votes <- table(namesD)
# picking randomly a choice if equal number of choices
obs[,response] <- votes[votes==max(votes)]
return(list(class=obs, confidence=prop_votes[names(out_vote)]))
}
}
knn(df = swiss[-1,], k = 5, obs = swiss[1,], response = 'Infant.Mortality')
knn(df = iris[-1,], k = 10, obs = iris[100,], response = 'Petal.Width')
knn <- function(df, k, obs, response){
# implementing k-nearest neighbours. This function can work with
# categorical or numeric response. It can only predict one observation
# at a time.
obs <- select(obs,-matches(response))
isNum <- sapply(obs,is.numeric)
obs <- select(obs, which(isNum))
num_features <- colnames(df)[unname(sapply(df,is.numeric))]  # Numeric variables only
num_features <- num_features[num_features != response]
N <- nrow(df)  # Number of observations
p <- length(num_features)  # Number of input variables
X <- as.matrix(df[,num_features])  # Transforming into matrix
X <- prcomp(t(X))$rotation
Y <- as.matrix(df[,response]) # Response variable as matrix
dimnames(Y)[[2]] <- list(response)[,c(1,2)]
O <- matrix(unlist(rep(unname(obs),N)),byrow=T,nrow=N)  # obs values as matrix
dimnames(O)[[1]] = dimnames(Y)[[1]] <- dimnames(X)[[1]]  # Keeping same sample names
dimnames(O)[[2]] <- dimnames(X)[[2]] # Keeping same feature names
D <- (X-O)^2 %*% matrix(rep(1, p), ncol = 1)  # Euclidean distance calculation
D <- D[order(D[,1]),][1:k]  # Filtering k-nearest neighbours
if(is.numeric(df[,response])){  # regression mode
obs[,response] <- mean(Y[names(D),])
return(obs)
} else{  # classification mode
votes <- table(namesD)
# picking randomly a choice if equal number of choices
obs[,response] <- votes[votes==max(votes)]
return(list(class=obs, confidence=prop_votes[names(out_vote)]))
}
}
knn(df = swiss[-1,], k = 5, obs = swiss[1,], response = 'Infant.Mortality')
knn <- function(df, k, obs, response){
# implementing k-nearest neighbours. This function can work with
# categorical or numeric response. It can only predict one observation
# at a time.
obs <- select(obs,-matches(response))
isNum <- sapply(obs,is.numeric)
obs <- select(obs, which(isNum))
num_features <- colnames(df)[unname(sapply(df,is.numeric))]  # Numeric variables only
num_features <- num_features[num_features != response]
N <- nrow(df)  # Number of observations
p <- length(num_features)  # Number of input variables
X <- as.matrix(df[,num_features])  # Transforming into matrix
X <- prcomp(t(X))$rotation[,c(1,2)]
Y <- as.matrix(df[,response]) # Response variable as matrix
dimnames(Y)[[2]] <- list(response)
O <- matrix(unlist(rep(unname(obs),N)),byrow=T,nrow=N)  # obs values as matrix
dimnames(O)[[1]] = dimnames(Y)[[1]] <- dimnames(X)[[1]]  # Keeping same sample names
dimnames(O)[[2]] <- dimnames(X)[[2]] # Keeping same feature names
D <- (X-O)^2 %*% matrix(rep(1, p), ncol = 1)  # Euclidean distance calculation
D <- D[order(D[,1]),][1:k]  # Filtering k-nearest neighbours
if(is.numeric(df[,response])){  # regression mode
obs[,response] <- mean(Y[names(D),])
return(obs)
} else{  # classification mode
votes <- table(namesD)
# picking randomly a choice if equal number of choices
obs[,response] <- votes[votes==max(votes)]
return(list(class=obs, confidence=prop_votes[names(out_vote)]))
}
}
knn(df = swiss[-1,], k = 5, obs = swiss[1,], response = 'Infant.Mortality')
knn(df = iris[-1,], k = 10, obs = iris[100,], response = 'Petal.Width')
knn <- function(df, k, obs, response){
# implementing k-nearest neighbours. This function can work with
# categorical or numeric response. It can only predict one observation
# at a time.
obs <- select(obs,-matches(response))
isNum <- sapply(obs,is.numeric)
obs <- select(obs, which(isNum))
num_features <- colnames(df)[unname(sapply(df,is.numeric))]  # Numeric variables only
num_features <- num_features[num_features != response]
N <- nrow(df)  # Number of observations
p <- length(num_features)  # Number of input variables
X <- as.matrix(df[,num_features])  # Transforming into matrix
X <- prcomp(t(X))$rotation[,c(1,2)]; p <- 2
Y <- as.matrix(df[,response]) # Response variable as matrix
dimnames(Y)[[2]] <- list(response)
O <- matrix(unlist(rep(unname(obs),N)),byrow=T,nrow=N)  # obs values as matrix
dimnames(O)[[1]] = dimnames(Y)[[1]] <- dimnames(X)[[1]]  # Keeping same sample names
dimnames(O)[[2]] <- dimnames(X)[[2]] # Keeping same feature names
D <- (X-O)^2 %*% matrix(rep(1, p), ncol = 1)  # Euclidean distance calculation
D <- D[order(D[,1]),][1:k]  # Filtering k-nearest neighbours
if(is.numeric(df[,response])){  # regression mode
obs[,response] <- mean(Y[names(D),])
return(obs)
} else{  # classification mode
votes <- table(namesD)
# picking randomly a choice if equal number of choices
obs[,response] <- votes[votes==max(votes)]
return(list(class=obs, confidence=prop_votes[names(out_vote)]))
}
}
knn(df = swiss[-1,], k = 5, obs = swiss[1,], response = 'Infant.Mortality')
knn(df = iris[-1,], k = 10, obs = iris[100,], response = 'Petal.Width')
df <- iris
df <- iris[-1,]
obs <- iris[1,]
response <- "Species"
response <- "Petal.Width"
obs <- select(obs,-matches(response))
isNum <- sapply(obs,is.numeric)
obs <- select(obs, which(isNum))
num_features <- colnames(df)[unname(sapply(df,is.numeric))]  # Numeric variables only
num_features <- num_features[num_features != response]
N <- nrow(df)  # Number of observations
p <- length(num_features)  # Number of input variables
X <- as.matrix(df[,num_features])  # Transforming into matrix
X
prcomp(t(X))$rotation
prcomp(t(X))$rotation[,c(1,2)]
X <- prcomp(t(X))$rotation[,c(1,2)]; p <- 2
Y <- as.matrix(df[,response]) # Response variable as matrix
dimnames(Y)[[2]] <- list(response)
O <- matrix(unlist(rep(unname(obs),N)),byrow=T,nrow=N)  # obs values as matrix
dimnames(O)[[1]] = dimnames(Y)[[1]] <- dimnames(X)[[1]]  # Keeping same sample names
dimnames(O)[[2]] <- dimnames(X)[[2]] # Keeping same feature names
dimnames(O)
dimnames(X)[[2]]
dimnames(O)[[2]] <- dimnames(X)[[2]] # Keeping same feature names
dimnames(O)
O
