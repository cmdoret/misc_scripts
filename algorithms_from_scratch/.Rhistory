knn(df = swiss[-1,], k = 20, obs = swiss[1,], softmax = F, response = 'Infant.Mortality')
knn(df = swiss[-1,], k = 2, obs = swiss[1,], softmax = F, response = 'Infant.Mortality')
knn(df = swiss[-1,], k = 2, obs = swiss[1,], softmax = T, response = 'Infant.Mortality')
knn(df = swiss[-1,], k = 46, obs = swiss[1,], softmax = T, response = 'Infant.Mortality')
knn(df = swiss[-1,], k = 46, obs = swiss[1,], softmax = F, response = 'Infant.Mortality')
knn(df = swiss[-1,], k = 46, obs = swiss[1,], softmax = F, response = 'Infant.Mortality')
knn(df = swiss[-1,], k = 46, obs = swiss[1,], softmax = T, response = 'Infant.Mortality')
print(sum(weight))
weight
sum(rep(1,3))
knn <- function(df, k, softmax=F, obs, response){
# implementing k-nearest neighbours. This function can work with
# categorical or numeric response. It can only predict one observation
# at a time. Possibility to chose between softmax weighted votes or simple
# cut-off at k-nearest neighbours.
obs <- select(obs,-matches(response))
num_features <- colnames(df)[unname(sapply(df,is.numeric))]  # Numeric variables only
num_features <- num_features[num_features != response]
N <- nrow(df)  # Number of observations
p <- length(num_features)  # Number of input variables
X <- as.matrix(df[,num_features])  # Transforming into matrix
Y <- as.matrix(df[,response]) # Response variable as matrix
dimnames(Y)[[2]] <- list(response)
O <- matrix(unlist(rep(unname(obs),N)),byrow=T,nrow=N)  # obs values as matrix
dimnames(O)[[1]] = dimnames(Y)[[1]] <- dimnames(X)[[1]]  # Keeping same sample names
dimnames(O)[[2]] <- dimnames(X)[[2]] # Keeping same feature names
D <- (X-O)^2 %*% matrix(rep(1, p), ncol = 1)  # Euclidean distance calculation
D <- D[order(D[,1]),][1:k]  # Filtering k-nearest neighbours
if(is.numeric(df[,response])){  # regression mode
weight <- ifelse(softmax,yes = list(round(exp(-D)/sum(exp(-D)),3)),
no = list(rep(x = 1/k,times=k)))[[1]]
obs[,response] <- weight %*% as.matrix(Y[names(D),])
print(sum(weight))
return(obs)
} else{  # classification mode
votes <- table(D)
out_vote <- votes[votes==max(votes)]
out_vote <- out_vote[sample(x = 1:length(out_vote),size = 1)]
prop_votes <- sapply(votes,function(x) x/sum(votes))
# picking randomly a choice if equal number of choices
obs[,response] <- out_vote
return(list(class=obs, confidence=prop_votes[names(out_vote)]))
}
}
knn(df = swiss[-1,], k = 46, obs = swiss[1,], softmax = T, response = 'Infant.Mortality')
knn(df = swiss[-1,], k = 46, obs = swiss[1,], softmax = F, response = 'Infant.Mortality')
knn <- function(df, k, softmax=F, obs, response){
# implementing k-nearest neighbours. This function can work with
# categorical or numeric response. It can only predict one observation
# at a time. Possibility to chose between softmax weighted votes or simple
# cut-off at k-nearest neighbours.
obs <- select(obs,-matches(response))
num_features <- colnames(df)[unname(sapply(df,is.numeric))]  # Numeric variables only
num_features <- num_features[num_features != response]
N <- nrow(df)  # Number of observations
p <- length(num_features)  # Number of input variables
X <- as.matrix(df[,num_features])  # Transforming into matrix
Y <- as.matrix(df[,response]) # Response variable as matrix
dimnames(Y)[[2]] <- list(response)
O <- matrix(unlist(rep(unname(obs),N)),byrow=T,nrow=N)  # obs values as matrix
dimnames(O)[[1]] = dimnames(Y)[[1]] <- dimnames(X)[[1]]  # Keeping same sample names
dimnames(O)[[2]] <- dimnames(X)[[2]] # Keeping same feature names
D <- (X-O)^2 %*% matrix(rep(1, p), ncol = 1)  # Euclidean distance calculation
D <- D[order(D[,1]),][1:k]  # Filtering k-nearest neighbours
if(is.numeric(df[,response])){  # regression mode
weight <- ifelse(softmax,yes = list(round(exp(-D)/sum(exp(-D)),3)),
no = list(rep(x = 1/k,times=k)))[[1]]
obs[,response] <- weight %*% as.matrix(Y[names(D),])
print(weight)
return(obs)
} else{  # classification mode
votes <- table(D)
out_vote <- votes[votes==max(votes)]
out_vote <- out_vote[sample(x = 1:length(out_vote),size = 1)]
prop_votes <- sapply(votes,function(x) x/sum(votes))
# picking randomly a choice if equal number of choices
obs[,response] <- out_vote
return(list(class=obs, confidence=prop_votes[names(out_vote)]))
}
}
knn(df = swiss[-1,], k = 46, obs = swiss[1,], softmax = F, response = 'Infant.Mortality')
knn(df = swiss[-1,], k = 46, obs = swiss[1,], softmax = T, response = 'Infant.Mortality')
knn <- function(df, k, softmax=F, obs, response){
# implementing k-nearest neighbours. This function can work with
# categorical or numeric response. It can only predict one observation
# at a time. Possibility to chose between softmax weighted votes or simple
# cut-off at k-nearest neighbours.
obs <- select(obs,-matches(response))
num_features <- colnames(df)[unname(sapply(df,is.numeric))]  # Numeric variables only
num_features <- num_features[num_features != response]
N <- nrow(df)  # Number of observations
p <- length(num_features)  # Number of input variables
X <- as.matrix(df[,num_features])  # Transforming into matrix
Y <- as.matrix(df[,response]) # Response variable as matrix
dimnames(Y)[[2]] <- list(response)
O <- matrix(unlist(rep(unname(obs),N)),byrow=T,nrow=N)  # obs values as matrix
dimnames(O)[[1]] = dimnames(Y)[[1]] <- dimnames(X)[[1]]  # Keeping same sample names
dimnames(O)[[2]] <- dimnames(X)[[2]] # Keeping same feature names
D <- (X-O)^2 %*% matrix(rep(1, p), ncol = 1)  # Euclidean distance calculation
D <- D[order(D[,1]),][1:k]  # Filtering k-nearest neighbours
if(softmax){D <- 1/(1+exp(-(D-mean(D))/sd(D)))}
if(is.numeric(df[,response])){  # regression mode
weight <- ifelse(softmax,yes = list(round(exp(-D)/sum(exp(-D)),3)),
no = list(rep(x = 1/k,times=k)))[[1]]
obs[,response] <- weight %*% as.matrix(Y[names(D),])
print(weight)
return(obs)
} else{  # classification mode
votes <- table(D)
out_vote <- votes[votes==max(votes)]
out_vote <- out_vote[sample(x = 1:length(out_vote),size = 1)]
prop_votes <- sapply(votes,function(x) x/sum(votes))
# picking randomly a choice if equal number of choices
obs[,response] <- out_vote
return(list(class=obs, confidence=prop_votes[names(out_vote)]))
}
}
knn(df = swiss[-1,], k = 46, obs = swiss[1,], softmax = T, response = 'Infant.Mortality')
knn(df = swiss[-1,], k = 46, obs = swiss[1,], softmax = T, response = 'Infant.Mortality')
knn(df = swiss[-1,], k = 46, obs = swiss[1,], softmax = F, response = 'Infant.Mortality')
knn <- function(df, k, softmax=F, obs, response){
# implementing k-nearest neighbours. This function can work with
# categorical or numeric response. It can only predict one observation
# at a time. Possibility to chose between softmax weighted votes or simple
# cut-off at k-nearest neighbours.
obs <- select(obs,-matches(response))
num_features <- colnames(df)[unname(sapply(df,is.numeric))]  # Numeric variables only
num_features <- num_features[num_features != response]
N <- nrow(df)  # Number of observations
p <- length(num_features)  # Number of input variables
X <- as.matrix(df[,num_features])  # Transforming into matrix
Y <- as.matrix(df[,response]) # Response variable as matrix
dimnames(Y)[[2]] <- list(response)
O <- matrix(unlist(rep(unname(obs),N)),byrow=T,nrow=N)  # obs values as matrix
dimnames(O)[[1]] = dimnames(Y)[[1]] <- dimnames(X)[[1]]  # Keeping same sample names
dimnames(O)[[2]] <- dimnames(X)[[2]] # Keeping same feature names
D <- (X-O)^2 %*% matrix(rep(1, p), ncol = 1)  # Euclidean distance calculation
D <- D[order(D[,1]),][1:k]  # Filtering k-nearest neighbours
if(softmax){D <- 1/(1+exp(-(D-mean(D))/sd(D)))}
if(is.numeric(df[,response])){  # regression mode
weight <- ifelse(softmax,yes = list(round(exp(-D)/sum(exp(-D)),3)),
no = list(rep(x = 1/k,times=k)))[[1]]
obs[,response] <- weight %*% as.matrix(Y[names(D),])
return(obs)
} else{  # classification mode
votes <- table(D)
out_vote <- votes[votes==max(votes)]
out_vote <- out_vote[sample(x = 1:length(out_vote),size = 1)]
prop_votes <- sapply(votes,function(x) x/sum(votes))
# picking randomly a choice if equal number of choices
obs[,response] <- out_vote
return(list(class=obs, confidence=prop_votes[names(out_vote)]))
}
}
knn(df = swiss[-1,], k = 46, obs = swiss[1,], softmax = F, response = 'Infant.Mortality')
swiss[1,]
knn(df = swiss[-1,], k = 46, obs = swiss[1,], softmax = T, response = 'Infant.Mortality')
knn(df = swiss[-1,], k = 5, obs = swiss[1,], softmax = T, response = 'Infant.Mortality')
knn(df = swiss[-1,], k = 5, obs = swiss[1,], softmax = F, response = 'Infant.Mortality')
table(D)
knn(df = iris[-1,], k = 5, obs = iris[1,], softmax = F, response = 'Species')
knn(df = iris[-1,], k = 5, obs = iris[1,], softmax = F, response = 'Petal.Length')
knn(df = iris[-1,], k = 5, obs = iris[1,], softmax = F, response = 'Petal.Width')
dimnames(X)[[2]]
dimnames(X)[[2]]
colnames(swiss)
str(dimnames(X)[[2]])
str(colnames(swiss))
select(colnames(df,-matches(response))
)
select(colnames(df,-matches(response)))
select(colnames(df),-matches(response))
colnames(df)[colnames(df) %in% response]
colnames(df)[-(colnames(df) %in% response)]
colnames(df)[-(colnames(df) == response)]
colnames(df)[colnames(df) != response]
dimnames(X)[[2]]
knn <- function(df, k, softmax=F, obs, response){
# implementing k-nearest neighbours. This function can work with
# categorical or numeric response. It can only predict one observation
# at a time. Possibility to chose between softmax weighted votes or simple
# cut-off at k-nearest neighbours.
obs <- select(obs,-matches(response))
num_features <- colnames(df)[unname(sapply(df,is.numeric))]  # Numeric variables only
num_features <- num_features[num_features != response]
N <- nrow(df)  # Number of observations
p <- length(num_features)  # Number of input variables
X <- as.matrix(df[,num_features])  # Transforming into matrix
Y <- as.matrix(df[,response]) # Response variable as matrix
dimnames(Y)[[2]] <- list(response)
O <- matrix(unlist(rep(unname(obs),N)),byrow=T,nrow=N)  # obs values as matrix
dimnames(O)[[1]] = dimnames(Y)[[1]] <- dimnames(X)[[1]]  # Keeping same sample names
dimnames(O)[[2]] <- colnames(df)[colnames(df) != response] # Keeping same feature names
D <- (X-O)^2 %*% matrix(rep(1, p), ncol = 1)  # Euclidean distance calculation
D <- D[order(D[,1]),][1:k]  # Filtering k-nearest neighbours
if(softmax){D <- 1/(1+exp(-(D-mean(D))/sd(D)))} # Normalize distance to reduce impact of outliers
if(is.numeric(df[,response])){  # regression mode
weight <- ifelse(softmax,yes = list(round(exp(-D)/sum(exp(-D)),3)),
no = list(rep(x = 1/k,times=k)))[[1]]
# weights are either all the same or defined by softmax function
obs[,response] <- weight %*% as.matrix(Y[names(D),])
return(obs)
} else{  # classification mode
votes <- table(namesD)
if(softmax){
out_vote <- votes[votes==max(votes)]
out_vote <- out_vote[sample(x = 1:length(out_vote),size = 1)]
prop_votes <- sapply(votes,function(x) x/sum(votes))
}
# picking randomly a choice if equal number of choices
obs[,response] <- out_vote
return(list(class=obs, confidence=prop_votes[names(out_vote)]))
}
}
knn(df = iris[-1,], k = 5, obs = iris[1,], softmax = F, response = 'Petal.Width')
knn <- function(df, k, softmax=F, obs, response){
# implementing k-nearest neighbours. This function can work with
# categorical or numeric response. It can only predict one observation
# at a time. Possibility to chose between softmax weighted votes or simple
# cut-off at k-nearest neighbours.
obs <- select(obs,-matches(response))
num_features <- colnames(df)[unname(sapply(df,is.numeric))]  # Numeric variables only
num_features <- num_features[num_features != response]
N <- nrow(df)  # Number of observations
p <- length(num_features)  # Number of input variables
X <- as.matrix(df[,num_features])  # Transforming into matrix
Y <- as.matrix(df[,response]) # Response variable as matrix
dimnames(Y)[[2]] <- list(response)
O <- matrix(unlist(rep(unname(obs),N)),byrow=T,nrow=N)  # obs values as matrix
dimnames(O)[[1]] = dimnames(Y)[[1]] <- dimnames(X)[[1]]  # Keeping same sample names
dimnames(O)[[2]] <- colnames(df)[colnames(df) != response] # Keeping same feature names
print(X);print(O)
D <- (X-O)^2 %*% matrix(rep(1, p), ncol = 1)  # Euclidean distance calculation
D <- D[order(D[,1]),][1:k]  # Filtering k-nearest neighbours
if(softmax){D <- 1/(1+exp(-(D-mean(D))/sd(D)))} # Normalize distance to reduce impact of outliers
if(is.numeric(df[,response])){  # regression mode
weight <- ifelse(softmax,yes = list(round(exp(-D)/sum(exp(-D)),3)),
no = list(rep(x = 1/k,times=k)))[[1]]
# weights are either all the same or defined by softmax function
obs[,response] <- weight %*% as.matrix(Y[names(D),])
return(obs)
} else{  # classification mode
votes <- table(namesD)
if(softmax){
out_vote <- votes[votes==max(votes)]
out_vote <- out_vote[sample(x = 1:length(out_vote),size = 1)]
prop_votes <- sapply(votes,function(x) x/sum(votes))
}
# picking randomly a choice if equal number of choices
obs[,response] <- out_vote
return(list(class=obs, confidence=prop_votes[names(out_vote)]))
}
}
knn(df = iris[-1,], k = 5, obs = iris[1,], softmax = F, response = 'Petal.Width')
knn <- function(df, k, softmax=F, obs, response){
# implementing k-nearest neighbours. This function can work with
# categorical or numeric response. It can only predict one observation
# at a time. Possibility to chose between softmax weighted votes or simple
# cut-off at k-nearest neighbours.
obs <- select(obs,-matches(response))
isNum <- sapply(obs,is.numeric())
obs <- select(obs, which(isNum))
num_features <- colnames(df)[unname(sapply(df,is.numeric))]  # Numeric variables only
num_features <- num_features[num_features != response]
N <- nrow(df)  # Number of observations
p <- length(num_features)  # Number of input variables
X <- as.matrix(df[,num_features])  # Transforming into matrix
Y <- as.matrix(df[,response]) # Response variable as matrix
dimnames(Y)[[2]] <- list(response)
O <- matrix(unlist(rep(unname(obs),N)),byrow=T,nrow=N)  # obs values as matrix
dimnames(O)[[1]] = dimnames(Y)[[1]] <- dimnames(X)[[1]]  # Keeping same sample names
dimnames(O)[[2]] <- dimnames(X)[[2]] # Keeping same feature names
D <- (X-O)^2 %*% matrix(rep(1, p), ncol = 1)  # Euclidean distance calculation
D <- D[order(D[,1]),][1:k]  # Filtering k-nearest neighbours
if(softmax){D <- 1/(1+exp(-(D-mean(D))/sd(D)))} # Normalize distance to reduce impact of outliers
if(is.numeric(df[,response])){  # regression mode
weight <- ifelse(softmax,yes = list(round(exp(-D)/sum(exp(-D)),3)),
no = list(rep(x = 1/k,times=k)))[[1]]
# weights are either all the same or defined by softmax function
obs[,response] <- weight %*% as.matrix(Y[names(D),])
return(obs)
} else{  # classification mode
votes <- table(namesD)
if(softmax){
out_vote <- votes[votes==max(votes)]
out_vote <- out_vote[sample(x = 1:length(out_vote),size = 1)]
prop_votes <- sapply(votes,function(x) x/sum(votes))
}
# picking randomly a choice if equal number of choices
obs[,response] <- out_vote
return(list(class=obs, confidence=prop_votes[names(out_vote)]))
}
}
knn(df = iris[-1,], k = 5, obs = iris[1,], softmax = F, response = 'Petal.Width')
knn <- function(df, k, softmax=F, obs, response){
# implementing k-nearest neighbours. This function can work with
# categorical or numeric response. It can only predict one observation
# at a time. Possibility to chose between softmax weighted votes or simple
# cut-off at k-nearest neighbours.
obs <- select(obs,-matches(response))
isNum <- sapply(obs,is.numeric)
obs <- select(obs, which(isNum))
num_features <- colnames(df)[unname(sapply(df,is.numeric))]  # Numeric variables only
num_features <- num_features[num_features != response]
N <- nrow(df)  # Number of observations
p <- length(num_features)  # Number of input variables
X <- as.matrix(df[,num_features])  # Transforming into matrix
Y <- as.matrix(df[,response]) # Response variable as matrix
dimnames(Y)[[2]] <- list(response)
O <- matrix(unlist(rep(unname(obs),N)),byrow=T,nrow=N)  # obs values as matrix
dimnames(O)[[1]] = dimnames(Y)[[1]] <- dimnames(X)[[1]]  # Keeping same sample names
dimnames(O)[[2]] <- dimnames(X)[[2]] # Keeping same feature names
D <- (X-O)^2 %*% matrix(rep(1, p), ncol = 1)  # Euclidean distance calculation
D <- D[order(D[,1]),][1:k]  # Filtering k-nearest neighbours
if(softmax){D <- 1/(1+exp(-(D-mean(D))/sd(D)))} # Normalize distance to reduce impact of outliers
if(is.numeric(df[,response])){  # regression mode
weight <- ifelse(softmax,yes = list(round(exp(-D)/sum(exp(-D)),3)),
no = list(rep(x = 1/k,times=k)))[[1]]
# weights are either all the same or defined by softmax function
obs[,response] <- weight %*% as.matrix(Y[names(D),])
return(obs)
} else{  # classification mode
votes <- table(namesD)
if(softmax){
out_vote <- votes[votes==max(votes)]
out_vote <- out_vote[sample(x = 1:length(out_vote),size = 1)]
prop_votes <- sapply(votes,function(x) x/sum(votes))
}
# picking randomly a choice if equal number of choices
obs[,response] <- out_vote
return(list(class=obs, confidence=prop_votes[names(out_vote)]))
}
}
knn(df = iris[-1,], k = 5, obs = iris[1,], softmax = F, response = 'Petal.Width')
iris[1,]
knn(df = iris[-1,], k = 50, obs = iris[1,], softmax = F, response = 'Petal.Width')
knn(df = iris[-1,], k = 50, obs = iris[1,], softmax = T, response = 'Petal.Width')
knn(df = iris[-1,], k = 1, obs = iris[1,], softmax = T, response = 'Petal.Width')
knn(df = iris[-1,], k = 2, obs = iris[1,], softmax = T, response = 'Petal.Width')
knn(df = iris[-1,], k = 3, obs = iris[1,], softmax = T, response = 'Petal.Width')
knn(df = iris[-1,], k = 1, obs = iris[1,], softmax = F, response = 'Petal.Width')
knn(df = iris[-1,], k = 2, obs = iris[1,], softmax = F, response = 'Petal.Width')
knn(df = iris[-1,], k = 3, obs = iris[1,], softmax = F, response = 'Petal.Width')
knn(df = iris[-1,], k = 4, obs = iris[1,], softmax = F, response = 'Petal.Width')
knn(df = iris[-1,], k = 10, obs = iris[1,], softmax = F, response = 'Petal.Width')
knn(df = iris[-1,], k = 10, obs = iris[1,], softmax = F, response = 'Petal.Width')
knn(df = iris[-1,], k = 10, obs = iris[1,], softmax = F, response = 'Petal.Width')
knn(df = swiss[-1,], k = 5, obs = swiss[1,], softmax = F, response = 'Infant.Mortality')
knn <- function(df, k, softmax=F, obs, response){
# implementing k-nearest neighbours. This function can work with
# categorical or numeric response. It can only predict one observation
# at a time. Possibility to chose between softmax weighted votes or simple
# cut-off at k-nearest neighbours.
obs <- select(obs,-matches(response))
isNum <- sapply(obs,is.numeric)
obs <- select(obs, which(isNum))
num_features <- colnames(df)[unname(sapply(df,is.numeric))]  # Numeric variables only
num_features <- num_features[num_features != response]
N <- nrow(df)  # Number of observations
p <- length(num_features)  # Number of input variables
X <- as.matrix(df[,num_features])  # Transforming into matrix
Y <- as.matrix(df[,response]) # Response variable as matrix
dimnames(Y)[[2]] <- list(response)
O <- matrix(unlist(rep(unname(obs),N)),byrow=T,nrow=N)  # obs values as matrix
dimnames(O)[[1]] = dimnames(Y)[[1]] <- dimnames(X)[[1]]  # Keeping same sample names
dimnames(O)[[2]] <- dimnames(X)[[2]] # Keeping same feature names
D <- (X-O)^2 %*% matrix(rep(1, p), ncol = 1)  # Euclidean distance calculation
D <- D[order(D[,1]),][1:k]  # Filtering k-nearest neighbours
if(softmax){D <- 1/(1+exp(-(D-mean(D))/sd(D)))} # Normalize distance to reduce impact of outliers
if(is.numeric(df[,response])){  # regression mode
weight <- ifelse(softmax,yes = list(round(exp(-D)/sum(exp(-D)),3)),
no = list(rep(x = 1/k,times=k)))[[1]]
# weights are either all the same or defined by softmax function
obs[,response] <- weight %*% as.matrix(Y[names(D),])
return(obs)
} else{  # classification mode
votes <- table(namesD)
if(softmax){
out_vote <- votes[votes==max(votes)]
out_vote <- out_vote[sample(x = 1:length(out_vote),size = 1)]
prop_votes <- sapply(votes,function(x) x/sum(votes))
}
# picking randomly a choice if equal number of choices
obs[,response] <- out_vote
Sys.sleep(1)
return(list(class=obs, confidence=prop_votes[names(out_vote)]))
}
}
knn(df = iris[-1,], k = 10, obs = iris[1,], softmax = F, response = 'Petal.Width')
knn <- function(df, k, softmax=F, obs, response){
# implementing k-nearest neighbours. This function can work with
# categorical or numeric response. It can only predict one observation
# at a time. Possibility to chose between softmax weighted votes or simple
# cut-off at k-nearest neighbours.
obs <- select(obs,-matches(response))
isNum <- sapply(obs,is.numeric)
obs <- select(obs, which(isNum))
num_features <- colnames(df)[unname(sapply(df,is.numeric))]  # Numeric variables only
num_features <- num_features[num_features != response]
N <- nrow(df)  # Number of observations
p <- length(num_features)  # Number of input variables
X <- as.matrix(df[,num_features])  # Transforming into matrix
Y <- as.matrix(df[,response]) # Response variable as matrix
dimnames(Y)[[2]] <- list(response)
O <- matrix(unlist(rep(unname(obs),N)),byrow=T,nrow=N)  # obs values as matrix
dimnames(O)[[1]] = dimnames(Y)[[1]] <- dimnames(X)[[1]]  # Keeping same sample names
dimnames(O)[[2]] <- dimnames(X)[[2]] # Keeping same feature names
D <- (X-O)^2 %*% matrix(rep(1, p), ncol = 1)  # Euclidean distance calculation
D <- D[order(D[,1]),][1:k]  # Filtering k-nearest neighbours
if(softmax){D <- 1/(1+exp(-(D-mean(D))/sd(D)))} # Normalize distance to reduce impact of outliers
if(is.numeric(df[,response])){  # regression mode
weight <- ifelse(softmax,yes = list(round(exp(-D)/sum(exp(-D)),3)),
no = list(rep(x = 1/k,times=k)))[[1]]
# weights are either all the same or defined by softmax function
obs[,response] <- weight %*% as.matrix(Y[names(D),])
return(obs)
} else{  # classification mode
votes <- table(namesD)
if(softmax){
out_vote <- votes[votes==max(votes)]
out_vote <- out_vote[sample(x = 1:length(out_vote),size = 1)]
prop_votes <- sapply(votes,function(x) x/sum(votes))
}
# picking randomly a choice if equal number of choices
obs[,response] <- out_vote
Sys.sleep(4)
return(list(class=obs, confidence=prop_votes[names(out_vote)]))
}
}
knn(df = iris[-1,], k = 10, obs = iris[1,], softmax = F, response = 'Petal.Width')
knn <- function(df, k, softmax=F, obs, response){
# implementing k-nearest neighbours. This function can work with
# categorical or numeric response. It can only predict one observation
# at a time. Possibility to chose between softmax weighted votes or simple
# cut-off at k-nearest neighbours.
obs <- select(obs,-matches(response))
isNum <- sapply(obs,is.numeric)
obs <- select(obs, which(isNum))
num_features <- colnames(df)[unname(sapply(df,is.numeric))]  # Numeric variables only
num_features <- num_features[num_features != response]
N <- nrow(df)  # Number of observations
p <- length(num_features)  # Number of input variables
X <- as.matrix(df[,num_features])  # Transforming into matrix
Y <- as.matrix(df[,response]) # Response variable as matrix
dimnames(Y)[[2]] <- list(response)
O <- matrix(unlist(rep(unname(obs),N)),byrow=T,nrow=N)  # obs values as matrix
dimnames(O)[[1]] = dimnames(Y)[[1]] <- dimnames(X)[[1]]  # Keeping same sample names
dimnames(O)[[2]] <- dimnames(X)[[2]] # Keeping same feature names
Sys.sleep(4)
D <- (X-O)^2 %*% matrix(rep(1, p), ncol = 1)  # Euclidean distance calculation
D <- D[order(D[,1]),][1:k]  # Filtering k-nearest neighbours
if(softmax){D <- 1/(1+exp(-(D-mean(D))/sd(D)))} # Normalize distance to reduce impact of outliers
if(is.numeric(df[,response])){  # regression mode
weight <- ifelse(softmax,yes = list(round(exp(-D)/sum(exp(-D)),3)),
no = list(rep(x = 1/k,times=k)))[[1]]
# weights are either all the same or defined by softmax function
obs[,response] <- weight %*% as.matrix(Y[names(D),])
return(obs)
} else{  # classification mode
votes <- table(namesD)
if(softmax){
out_vote <- votes[votes==max(votes)]
out_vote <- out_vote[sample(x = 1:length(out_vote),size = 1)]
prop_votes <- sapply(votes,function(x) x/sum(votes))
}
# picking randomly a choice if equal number of choices
obs[,response] <- out_vote
return(list(class=obs, confidence=prop_votes[names(out_vote)]))
}
}
knn(df = iris[-1,], k = 10, obs = iris[1,], softmax = F, response = 'Petal.Width')
knn(df = iris[-1,], k = 10, obs = iris[1,], softmax = F, response = 'Petal.Width')
knn <- function(df, k, softmax=F, obs, response){
# implementing k-nearest neighbours. This function can work with
# categorical or numeric response. It can only predict one observation
# at a time. Possibility to chose between softmax weighted votes or simple
# cut-off at k-nearest neighbours.
obs <- select(obs,-matches(response))
isNum <- sapply(obs,is.numeric)
obs <- select(obs, which(isNum))
num_features <- colnames(df)[unname(sapply(df,is.numeric))]  # Numeric variables only
num_features <- num_features[num_features != response]
N <- nrow(df)  # Number of observations
p <- length(num_features)  # Number of input variables
X <- as.matrix(df[,num_features])  # Transforming into matrix
Y <- as.matrix(df[,response]) # Response variable as matrix
dimnames(Y)[[2]] <- list(response)
O <- matrix(unlist(rep(unname(obs),N)),byrow=T,nrow=N)  # obs values as matrix
dimnames(O)[[1]] = dimnames(Y)[[1]] <- dimnames(X)[[1]]  # Keeping same sample names
dimnames(O)[[2]] <- dimnames(X)[[2]] # Keeping same feature names
D <- (X-O)^2 %*% matrix(rep(1, p), ncol = 1)  # Euclidean distance calculation
D <- D[order(D[,1]),][1:k]  # Filtering k-nearest neighbours
if(softmax){D <- 1/(1+exp(-(D-mean(D))/sd(D)))} # Normalize distance to reduce impact of outliers
if(is.numeric(df[,response])){  # regression mode
weight <- ifelse(softmax,yes = list(round(exp(-D)/sum(exp(-D)),3)),
no = list(rep(x = 1/k,times=k)))[[1]]
# weights are either all the same or defined by softmax function
obs[,response] <- weight %*% as.matrix(Y[names(D),])
return(obs)
} else{  # classification mode
votes <- table(namesD)
if(softmax){
out_vote <- votes[votes==max(votes)]
out_vote <- out_vote[sample(x = 1:length(out_vote),size = 1)]
prop_votes <- sapply(votes,function(x) x/sum(votes))
}
# picking randomly a choice if equal number of choices
obs[,response] <- out_vote
return(list(class=obs, confidence=prop_votes[names(out_vote)]))
}
}
knn(df = iris[-1,], k = 10, obs = iris[1,], softmax = F, response = 'Petal.Width')
